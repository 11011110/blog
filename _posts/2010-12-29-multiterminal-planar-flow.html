---
layout: post
title:  'Multiterminal planar flow'
date:   2010-12-29 22:40:00
tags:   [graph algorithms, papers]
---
The main point of this post is to talk about two recent arXiv preprints on <a href="http://en.wikipedia.org/wiki/Maximum_flow_problem">maximum flow algorithms</a>, &quot;Multiple-Source Multiple-Sink Maximum Flow in Planar Graphs&quot; (<a href="http://arxiv.org/abs/1012.4767">arXiv:1012.4767</a>) by Yahav Nussbaum, and &quot;Multiple-Source Multiple-Sink Maximum Flow in Directed Planar Graphs in O(n<sup>3/2</sup>log n) Time&quot; (<a href="http://arxiv.org/abs/1012.5870">arXiv:1012.5870</a>) by Shay Mozes. However, first I&#x27;d like to set the stage by apologizing for some sloppy scholarship in the literature survey component of one of my own papers.
<br />

<br />
In my recent ISAAC paper &quot;Flows in One-Crossing-Minor-Free Graphs&quot; with Erin Chambers, we wrote &quot;Due to known <a href="http://en.wikipedia.org/wiki/Planar_separator_theorem">separator theorems</a> for minor-closed families, an algorithm of Johnson and Venkatesan can be applied to find flows in any minor-closed family in time O(n<sup>3/2</sup>log n)&quot;. But this appears to be overstated at best, and more likely just wrong. Johnson and Venkatesan&#x27;s paper is about flows in planar networks, and they use both simple-cycle separators (having the property that contracting one side of the separator preserves planarity) and the duality between cuts and paths in the case that the two flow terminals are on the outer face of a planar embedding. So simply having separators is not good enough; one needs other ideas to make their techniques work, and the analogues to those other ideas in more general minor-closed families do not seem to be present. Which means that finding a maximum flow algorithm for minor-closed families that is better than the algorithms for arbitrary sparse graphs remains an interesting open problem.
<br />

<br />
Now, on to planar multiple-source multiple-sink flow. One may well ask, why doesn&#x27;t one see more papers on flows with multiple sources and sinks? It seems a natural enough variation on the problem. The answer is that, when the class of input graphs is unrestricted and one has multiple sources, one can replace them by a single new source connected by infinite-capacity edges to the previous sources. And similarly multiple sinks can be replaced by a single new sink. So flow with multiple sources and sinks is equivalent to a conventional flow problem with a single source and a single sink in a graph with two new vertices. But of course adding these vertices could destroy planarity. The graphs formed by adding two vertices to a planar graph (the 2-apex graphs) are minor-closed, so if the remark in my paper had been accurate then we would already know how to solve planar multi-terminal flows in time O(n<sup>3/2</sup>log n). But the remark was not accurate and before these papers came out we didn&#x27;t know how to do that.
<br />

<br />
Both new papers use cycle separators, and both have a divide-and-conquer strategy in which the &quot;conquer&quot; steps use as a subproblem algorithms for flow in planar graphs with multiple sources and a single sink (or multiple sinks and a single source). This subproblem was solved independently and nearly simultaneously in two other recent arXiv preprints by Borradaile and Wulﬀ-Nilsen (<a href="http://arxiv.org/abs/1008.4966">arXiv:1008.4966</a>) and Klein and Mozes (<a href="http://arxiv.org/abs/1008.5332">arXiv:1008.5332</a>) and its solution is itself a cycle-separator-based divide and conquer, whose running time is O(n<sup>3/2</sup>log n). In Nussbaum&#x27;s algorithm, the outer level of divide and conquer multiplies the time by another log factor, while Mozes finds a way of avoiding this penalty.
<br />

<br />
In a little more detail, Nussbaum uses a weighted separator theorem that allows him to partition the input graph into subgraphs with a number of sources and sinks that decreases by a constant factor at each level of the recursion. Mozes instead uses an unweighted separator theorem that decreases the total number of vertices by a constant factor at each level but that may not evenly split the sources and the sinks.
<br />

<br />
After dividing the vertices by his separator, Nussbaum finds a maximum flow from the sources on the left side of the separator to the sinks on the right side (by combining a multiple-source single-sink flow from the sources to the separator and a single-source multiple-sink flow from the separator to the sinks, and then reconciling these two flows). The minimum cut corresponding to this maximum flow is itself a good separator, allowing Nussbaum to recurse on each side of the cut. Then he adds the cut edges back to the graph and finds the remaining flow from the sources on the right side of the separator to the sinks on the left (again, by combining multi-source single-sink and single-source multi-sink flow subproblems).
<br />

<br />
Mozes performs the recursion in a slightly different order: he first finds a maximum flow in each subproblem, and then finds and reconciles maximum flows from the subproblems to the separator vertices and vice versa. By doing things in this order, he avoids having to find a cut that is a good separator, allowing him to use unweighted separators. So he gets a time that can be analyzed by something resembling the recurrence T(n) = 2T(n/2) + O(n<sup>3/2</sup>log n), which solves to O(n<sup>3/2</sup>log n) without any additional logs.
<br />

<br />
As well as being interesting for their algorithmic results, to me this sequence of papers is interesting as a case study of the speed-up in research made possible by preprint services such as the arXiv. If Nussbaum had to wait for one conference reviewing cycle to find out about the multiple-source single-sink papers that he needed for his result, and then Mozes had to wait for another cycle for Nussbaum&#x27;s paper to come out before he could improve it, it would all have taken a much longer time. And the mistake in my own paper could have made the delay even longer, by tricking some conference program committee members into thinking Nussbaum and Mozes&#x27; results were already known. Because of the arXiv, these unnecessary delays can be avoided, speeding the pace of research.
<br /><br /><hr /><br />
<h3>Comments:</h3>
<div style="margin-left: 0em; padding-top: 1em;">
<b>ext_376356</b>: <br />
<b>2010-12-31T18:30:29Z</b><br />
Thanks for the post on this, David.  ArXiv certainly does seem to speed things up, but perhaps it isn&#x27;t quite universally used at the moment.  My only complaint would be that, with fixed conference deadlines, one can plan vacations a little more easily.  With arXiv, there&#x27;s no saying when papers will pop up and a group will throw themselves into work - I would guess that simultaneous discovery of results will get rarer and rarer.

I don&#x27;t think our community has a sense of how arXiv should be used or what it means - I know I certainly don&#x27;t.
</div>
<div style="margin-left: 0em; padding-top: 1em;">
<b>None</b>: <br />
<b>2011-01-02T19:30:17Z</b><br />
Thanks for your very detailed posts, David. Happy new year!

Aravind Srinivasan
</div>
<div style="margin-left: 0em; padding-top: 1em;">
<b>None</b>: <br />
<b>2011-01-03T08:41:06Z</b><br />
Thanks for the detailed overview of what has been going on; this clears up some confusion I was having. Happy new year!

Bart Jansen.
</div>

