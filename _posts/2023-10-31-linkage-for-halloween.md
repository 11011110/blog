---
layout: post
title: Linkage for Halloween
date: 2023-10-31 17:21
---
* [A 3 state, 3 symbol Turing Machine that cannot be proven to halt](https://www.sligocki.com/2023/10/16/bb-3-3-is-hard.html) <span style="white-space:nowrap">([$$\mathbb{M}$$](https://mathstodon.xyz/@11011110/111248650705734543),</span> [via](https://lobste.rs/s/d1piz2/bb_3_3_is_hard)), when starting on a blank tape, without solving a Collatz-like problem.

* [Wikipedia edit-a-thon at FOCS](https://thmatters.wordpress.com/2023/10/15/wikipedia-edit-a-thon-at-focs/) <span style="white-space:nowrap">([$$\mathbb{M}$$](https://mathstodon.xyz/@11011110/111253454448026599))</span>, the IEEE Symposium on Foundations of Computer Science, Monday November 6, in Santa Cruz. I'm not going to the conference but I wish the attendees well. If you are attending, feel free to ping [my Wikipedia talk page](https://en.wikipedia.org/wiki/User_talk:David_Eppstein) to bring my attention to the articles you're working on.

* [Science is a strong-link problem](https://www.experimental-history.com/p/science-is-a-strong-link-problem) <span style="white-space:nowrap">([$$\mathbb{M}$$](https://mathstodon.xyz/@11011110/111258753667063166),</span> [via](https://www.metafilter.com/200964/Cognitive-Bias-Situations-Matter-Pick-a-Noun-and-other-dead-ends)). Adam Mastroianni argues that, in the publication and grant review infrastructure of science, it is much more important to support the best work than it is to gatekeep and filter out the bottom. And more strongly, that the sort of gatekeeping that we routinely do is counterproductive, because it stifles the best work. The via link covers another side of the same story: the idea that a certain line of research (in psychology but the same could apply in many other fields) that has been hugely successful by the usual metrics of academia, and may have been based on fraud, could be totally erased from the academic record with nobody noticing its absence.

* [Use of AI for OEIS Submissions is Forbidden!](https://oeis.org/wiki/Use_of_AI_for_OEIS_Submissions_is_Forbidden) <span style="white-space:nowrap">([$$\mathbb{M}$$](https://mathstodon.xyz/@peterluschny/111269092548386862)).</span>

* [ArXiv gets infrastructure grant, to move its data to the cloud](https://news.cornell.edu/stories/2023/10/research-repository-arxiv-receives-10m-upgrades) <span style="white-space:nowrap">([$$\mathbb{M}$$](https://mathstodon.xyz/@Ianagol/111269036294724117))</span> It's a bit unfortunate that the only way to get this kind of funding to keep essential infrastructure like arXiv running is to pretend to do something innovative in how it runs, rather than just "please help us pay the bills to keep it running".

* [According to Yahoo News, Google secretly changed their ad-auction algorithm](https://finance.yahoo.com/news/google-changed-ad-auctions-raising-191333390.html) <span style="white-space:nowrap">([$$\mathbb{M}$$](https://mathstodon.xyz/@11011110/111281699338108839),</span> [via](https://news.ycombinator.com/item?id=37820192)) from a standard [second-price auction](https://en.wikipedia.org/wiki/Vickrey_auction) to one in which the winner was charged an inflated price, closer to the first-price bid. Doing this would have the appearance of bringing in more money for each auction because it charges advertisers more while still staying under the price they're willing to pay. But it has big downsides, once found out: it causes participants to be unwilling to bid their true price, and could cause significant fluctuation of bids well below their "true" values that leads to bringing in less money rather than more. Also, there's a pretty strong case that telling bidders you're running a second-price auction but then not doing that is fraud.

* [Monotone dualization](https://en.wikipedia.org/wiki/Monotone_dualization) <span style="white-space:nowrap">([$$\mathbb{M}$$](https://mathstodon.xyz/@11011110/111285543507787129))</span> encompasses a family of equivalent computational problems that include converting logical formulas between conjunctive normal form and disjunctive normal form, and listing all minimal hitting sets or all minimal set covers of a finite family of sets. It has many applications, one of the more famous but less serious being its use in the computational proof that there is no valid 16-clue sudoku puzzle. From the point of view of theoretical computer science it is interesting for another reason: It can be computed in quasi-polynomial time (in the combined input and output size) but neither a polynomial time algorithm nor a hardness reduction explaining why it might be non-polynomial is known. Now the subject of a new article on Wikipedia.

* [Ramen geometry puzzle](https://mathstodon.xyz/@Mark_Andrew_Gerads/111282585937392667) by Mark Andrew Gerads: if you have two square packs of ramen, and you split one of them into two equal rectangles, how big a circular pot do you need to hold them both?

* A mathematician with a similar name to mine has also studied Egyptian fractions <span style="white-space:nowrap">([$$\mathbb{M}$$](https://mathstodon.xyz/@11011110/111298160613513791))</span>: ring theorist Neil Epstein of George Mason University has two preprints on [Egyptian integral domains](https://arxiv.org/abs/2303.15685) and [Egyptian rings](https://arxiv.org/abs/2304.09845), systems of numbers for which every fraction can be written as a sum of unit fractions (possibly required to be distinct). See also [an earlier preprint by Guerrieri, Loper, and Oman](https://faculty.uccs.edu/goman/wp-content/uploads/sites/15/2023/04/JAA-submission.pdf). For instance, as GL&O note, $$\mathbb{Z}[\sqrt2]$$ is Egyptian, because one can replace any irrational numerator $$a+b\sqrt2$$ of a fraction by $$(a^2-2b^2)/(a-b\sqrt2)$$, express the new integer numerator as a sum of distinct unit integer fractions, and divide these all by the common denominator. However, $$\mathbb{Z}[x]$$ is not Egyptian, because there is no way of expressing $$x$$ as a sum of unit fractions.

* [A recent proposal in _Nature_ to allow multiple simultaneous journal submissions](https://www.nature.com/articles/d41586-023-03196-y) <span style="white-space:nowrap">([$$\mathbb{M}$$](https://mathstodon.xyz/@11011110/111304041670266825),</span>  [via](https://news.ycombinator.com/item?id=37850593)) must have been written by someone in a field where reviewing involves glancing over a paper and guessing at its significance rather than careful in-depth checking of mathematical proofs. Looking up the author, he appears to be a psychologist. Check.

* [New _Introduction to Probability for Computing_ book by Mor Harchol-Balter](https://www.cs.cmu.edu/~harchol/Probability/book.html) <span style="white-space:nowrap">([$$\mathbb{M}$$](https://fediscience.org/@fortnow/111307429950273574)).</span> Chapters free to download for personal use.

* [Plate lattice architectures](https://mathstodon.xyz/@alisonmartin57@mastodon.social/111243532000121153). Alison Martin makes 3d compressible and expandible structures from modular origami.

* Another Wikipedia editor, Thomas Preu, has finally started a Wikipedia article on [the Weisfeiler–Leman algorithm](https://en.wikipedia.org/wiki/Weisfeiler_Leman_graph_isomorphism_test) <span style="white-space:nowrap">([$$\mathbb{M}$$](https://mathstodon.xyz/@11011110/111322763033346127)).</span> This algorithm finds canonical colorings of 
$$k$$-tuples of vertices in graphs (which can in turn be used for graph isomorphism) by refining the labeling using the colors of the vertices' neighbors. I've had a browser bookmark for a long time (maybe years?) reminding me to do this, pointing to "[The Power of the Weisfeiler–Leman Algorithm to Decompose Graphs](https://arxiv.org/abs/1908.05268)", by Kiefer and Neuen, but it seems Preu has saved me the effort.

* Mike Vollmer asks: [What is the current state of the art for making beamer presentations accessible?](https://mathstodon.xyz/@vollmerm@types.pl/111325606368504773) So far there are no useful answers, so if you know, please let him know.

* [Egan conjecture holds](https://arxiv.org/abs/2310.10816) <span style="white-space:nowrap">([$$\mathbb{M}$$](https://mathstodon.xyz/@gregeganSF/111278352542761648)),</span> preprint by Sergei Drozdov. This is a tight inequality between the inradius, circumradius, and distance between centers of simplices in arbitrary dimensions, conjectured by Greg Egan in 2014.