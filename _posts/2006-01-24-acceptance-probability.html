---
layout: post
title:  'Acceptance probability'
date:   2006-01-24 20:30:00
tags:   [tools, language, conferences, spam]
---
A thought inspired by Cliff&#x27;s presentation at the SODA business meeting of keywords appearing often in accepted and rejected papers (with a lot of overlap between the two lists): someone should try teaching one of those Bayesian spam filters how to recognize good algorithms paper titles.  I&#x27;m not suggesting that any of the papers I saw for the STOC PC fell to the level of spam, and I don&#x27;t think any committee member would seriously think about using such a tool instead of their eyes, but it might be a useful early warning for authors that the committee is likely to be skeptical about their paper...
<br /><br /><hr /><br />
<h3>Comments:</h3>
<div style="margin-left: 0em; padding-top: 1em;">
<b>helger</b>: <br />
<b>2006-01-24T20:37:03Z</b><br />
So which keywords were good and which were bad?
</div>
<div style="margin-left: 3em; padding-top: 1em;">
<b>11011110</b>: <br />
<b>2006-01-24T23:38:16Z</b><br />
I didn&#x27;t record them, and I don&#x27;t know if Cliff is making his slides available, but there&#x27;s a brief recap in <a href="http://3dpancakes.typepad.com/ernie/2006/01/soda_business.html">Jeff&#x27;s business meeting report</a>:<blockquote>Good title: &quot;Approximate number games: A Way to Design improved graph algorithms without data bounds&quot; Bad title: &quot;Efficiently routing and scheduling random dynamic trees with complexity bounds.&quot;</blockquote>Of course, this doesn&#x27;t take length into account; e.g. long strings of modifiers are likely to indicate a highly specialized result not interesting to most of the committee.
</div>

